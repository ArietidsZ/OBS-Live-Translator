# Performance VRAM Profile (6GB+)
# Optimized for maximum quality and speed

[server]
host = "0.0.0.0"
port = 8080
monitoring_port = 8081
max_connections = 1000

[models]
# Full Whisper Large V3 for maximum accuracy
whisper_model = "openai/whisper-large-v3"  # Full model, not turbo
whisper_precision = "fp16"  # 3.1GB for best quality
whisper_decoder_layers = 32  # All layers for accuracy
nllb_model = "facebook/nllb-200-distilled-1.3B"
nllb_precision = "fp16"  # 2.6GB for quality
enable_ctranslate2 = false
enable_beam_search = true  # Maximum accuracy
beam_size = 5
temperature = 0.0  # Deterministic output

[streaming]
# Single stream with maximum quality
max_concurrent_streams = 1  # All resources for one perfect stream
default_priority = "normal"
buffer_size = 2048
max_latency_ms = 50

# Voice Activity Detection
enable_vad = true
vad_threshold = 0.4
min_speech_duration_ms = 150
min_silence_duration_ms = 80
skip_silence = true

# Maximum context for best understanding
chunk_duration_ms = 2000  # 2 second chunks
chunk_overlap_ms = 400    # Significant overlap
adaptive_chunking = true
min_chunk_ms = 500
max_chunk_ms = 2000

[cache]
# Large cache for performance
max_size_mb = 1024
eviction_policy = "adaptive"
enable_compression = false  # No compression for speed
enable_warming = true
warming_threshold = 0.6
predictive_warming = true
kv_cache_precision = "fp16"
max_context_tokens = 8192  # Maximum context window

[resources]
# Maximum resource utilization
max_cpu_percent = 80.0
max_memory_mb = 8192
max_gpu_memory_mb = 5500  # Includes TensorRT workspace
max_threads = 16
enable_dynamic_scaling = true

# Full GPU execution
enable_hybrid_execution = false

[optimization]
# All models in VRAM
enable_model_swapping = false

# No batching for lowest latency
max_batch_size = 1  # Process immediately
batch_timeout_ms = 20
priority_lanes = 4
critical_bypass_batch = true

# Full precision for accuracy
enable_mixed_precision = false  # Consistent FP16
whisper_compute_precision = "fp16"
nllb_compute_precision = "fp16"
enable_flash_attention = true  # Faster attention
enable_xformers = true  # Memory efficient attention

# Advanced optimizations
enable_flash_attention = true
enable_xformers = true
enable_torch_compile = true

[monitoring]
enable_metrics = true
enable_tracing = true
enable_profiling = true
metrics_interval_seconds = 15
log_level = "info"

[execution_providers]
# Maximum performance with TensorRT
providers = [
    "TensorrtExecutionProvider",
    "CUDAExecutionProvider",
    "CPUExecutionProvider"
]

# TensorRT settings
tensorrt_max_workspace_size = 1073741824  # 1GB workspace
tensorrt_fp16_enable = true
tensorrt_int8_enable = false  # FP16 only for quality
tensorrt_engine_cache_enable = true
tensorrt_engine_cache_path = "./tensorrt_engines"
tensorrt_dla_enable = false
tensorrt_builder_optimization_level = 5

# CUDA settings
cuda_arena_extend_strategy = "kNextPowerOfTwo"
cuda_mem_limit = 5500000000  # 5.5GB with overhead
enable_cuda_graph = true
cuda_graph_level = 2

# Advanced GPU settings
gpu_mem_growth = false
gpu_allow_growth = false
gpu_per_process_memory_fraction = 0.9