# Performance VRAM Profile (6GB+)
# Optimized for maximum quality and speed

[server]
host = "0.0.0.0"
port = 8080
monitoring_port = 8081
max_connections = 1000

[models]
# Whisper V3 Turbo with INT8 for efficiency
whisper_model = "openai/whisper-large-v3-turbo"
whisper_precision = "int8"  # 809MB
whisper_decoder_layers = 4
nllb_model = "facebook/nllb-200-distilled-1.3B"  # Larger model for better quality
nllb_precision = "int8"  # 1.3GB
enable_ctranslate2 = false  # Direct ONNX for INT8

[streaming]
# Safe concurrent streams (with full overhead)
max_concurrent_streams = 4  # Conservative for stability
default_priority = "normal"
buffer_size = 2048
max_latency_ms = 50

# Voice Activity Detection
enable_vad = true
vad_threshold = 0.4
min_speech_duration_ms = 150
min_silence_duration_ms = 80
skip_silence = true

# Larger chunks for better context
chunk_duration_ms = 1000
chunk_overlap_ms = 200
adaptive_chunking = true
min_chunk_ms = 500
max_chunk_ms = 2000

[cache]
# Large cache for performance
max_size_mb = 1024
eviction_policy = "adaptive"
enable_compression = false  # No compression for speed
enable_warming = true
warming_threshold = 0.6
predictive_warming = true
kv_cache_precision = "fp16"
max_context_tokens = 4096

[resources]
# Maximum resource utilization
max_cpu_percent = 80.0
max_memory_mb = 8192
max_gpu_memory_mb = 5500  # Includes TensorRT workspace
max_threads = 16
enable_dynamic_scaling = true

# Full GPU execution
enable_hybrid_execution = false

[optimization]
# All models in VRAM
enable_model_swapping = false

# Conservative batching for stability
max_batch_size = 4  # Reduced for memory safety
batch_timeout_ms = 20
priority_lanes = 4
critical_bypass_batch = true

# Mixed precision computation
enable_mixed_precision = true
whisper_compute_precision = "fp16"  # FP16 compute with INT8 weights
nllb_compute_precision = "fp16"  # FP16 compute with INT8 weights

# Advanced optimizations
enable_flash_attention = true
enable_xformers = true
enable_torch_compile = true

[monitoring]
enable_metrics = true
enable_tracing = true
enable_profiling = true
metrics_interval_seconds = 15
log_level = "info"

[execution_providers]
# Maximum performance with TensorRT
providers = [
    "TensorrtExecutionProvider",
    "CUDAExecutionProvider",
    "CPUExecutionProvider"
]

# TensorRT settings
tensorrt_max_workspace_size = 1073741824  # 1GB workspace
tensorrt_fp16_enable = true
tensorrt_int8_enable = false  # FP16 only for quality
tensorrt_engine_cache_enable = true
tensorrt_engine_cache_path = "./tensorrt_engines"
tensorrt_dla_enable = false
tensorrt_builder_optimization_level = 5

# CUDA settings
cuda_arena_extend_strategy = "kNextPowerOfTwo"
cuda_mem_limit = 5500000000  # 5.5GB with overhead
enable_cuda_graph = true
cuda_graph_level = 2

# Advanced GPU settings
gpu_mem_growth = false
gpu_allow_growth = false
gpu_per_process_memory_fraction = 0.9